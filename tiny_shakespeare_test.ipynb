{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J2E3Pt93v4P",
        "outputId": "50fa3a01-7220-4a6f-e945-cf21e0963f3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 689, done.\u001b[K\n",
            "remote: Total 689 (delta 0), reused 0 (delta 0), pack-reused 689 (from 1)\u001b[K\n",
            "Receiving objects: 100% (689/689), 975.24 KiB | 16.25 MiB/s, done.\n",
            "Resolving deltas: 100% (382/382), done.\n",
            "/content/nanoGPT\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git\n",
        "%cd nanoGPT\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile student_model.py\n",
        "\"\"\"\n",
        "RWKV-inspired Student model compatible with nanoGPT's training infrastructure.\n",
        "GPU-optimized with vectorized WKV computation.\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class StudentConfig:\n",
        "  block_size: int = 1024\n",
        "  vocab_size: int = 50304\n",
        "  n_layer: int = 5\n",
        "  n_embd: int = 128\n",
        "  dropout: float = 0.1\n",
        "  bias: bool = False\n",
        "\n",
        "\n",
        "class RWKVBlock(nn.Module):\n",
        "  def __init__(self, config, layer_id):\n",
        "      super().__init__()\n",
        "      dim = config.n_embd\n",
        "      self.layer_id = layer_id\n",
        "\n",
        "      self.ln1 = nn.LayerNorm(dim)\n",
        "      self.time_decay = nn.Parameter(torch.ones(dim) * -5.0)\n",
        "      self.time_first = nn.Parameter(torch.ones(dim) * math.log(0.3))\n",
        "\n",
        "      self.time_mix_k = nn.Parameter(torch.ones(1, 1, dim) * 0.5)\n",
        "      self.time_mix_v = nn.Parameter(torch.ones(1, 1, dim) * 0.5)\n",
        "      self.time_mix_r = nn.Parameter(torch.ones(1, 1, dim) * 0.5)\n",
        "\n",
        "      self.key = nn.Linear(dim, dim, bias=False)\n",
        "      self.value = nn.Linear(dim, dim, bias=False)\n",
        "      self.receptance = nn.Linear(dim, dim, bias=False)\n",
        "      self.output = nn.Linear(dim, dim, bias=False)\n",
        "\n",
        "      self.ln2 = nn.LayerNorm(dim)\n",
        "      self.channel_mix_k = nn.Parameter(torch.ones(1, 1, dim) * 0.5)\n",
        "      self.channel_mix_r = nn.Parameter(torch.ones(1, 1, dim) * 0.5)\n",
        "\n",
        "      hidden = int(dim * 2.5)\n",
        "      self.ffn_key = nn.Linear(dim, hidden, bias=False)\n",
        "      self.ffn_value = nn.Linear(hidden, dim, bias=False)\n",
        "      self.ffn_receptance = nn.Linear(dim, dim, bias=False)\n",
        "\n",
        "      self.drop = nn.Dropout(config.dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "      B, T, D = x.shape\n",
        "\n",
        "      residual = x\n",
        "      x_norm = self.ln1(x)\n",
        "      x_prev = F.pad(x_norm, (0, 0, 1, 0))[:, :-1, :]\n",
        "\n",
        "      xk = x_norm * self.time_mix_k + x_prev * (1 - self.time_mix_k)\n",
        "      xv = x_norm * self.time_mix_v + x_prev * (1 - self.time_mix_v)\n",
        "      xr = x_norm * self.time_mix_r + x_prev * (1 - self.time_mix_r)\n",
        "\n",
        "      k = self.key(xk)\n",
        "      v = self.value(xv)\n",
        "      r = torch.sigmoid(self.receptance(xr))\n",
        "\n",
        "      wkv = self.wkv_compute(k, v)\n",
        "      x = residual + self.drop(self.output(r * wkv))\n",
        "\n",
        "      residual = x\n",
        "      x_norm = self.ln2(x)\n",
        "      x_prev = F.pad(x_norm, (0, 0, 1, 0))[:, :-1, :]\n",
        "\n",
        "      xk = x_norm * self.channel_mix_k + x_prev * (1 - self.channel_mix_k)\n",
        "      xr = x_norm * self.channel_mix_r + x_prev * (1 - self.channel_mix_r)\n",
        "\n",
        "      k = torch.square(F.relu(self.ffn_key(xk)))\n",
        "      kv = self.ffn_value(k)\n",
        "      r = torch.sigmoid(self.ffn_receptance(xr))\n",
        "\n",
        "      x = residual + self.drop(r * kv)\n",
        "      return x\n",
        "\n",
        "  def wkv_compute(self, k, v):\n",
        "      \"\"\"Vectorized WKV computation - GPU friendly.\"\"\"\n",
        "      B, T, D = k.shape\n",
        "      w = torch.exp(-torch.exp(self.time_decay))  # (D,)\n",
        "      u = torch.exp(self.time_first)  # (D,)\n",
        "\n",
        "      ek = torch.exp(k)  # (B, T, D)\n",
        "      ekv = ek * v  # (B, T, D)\n",
        "\n",
        "      # Build powers of w for rescaling: w^0, w^1, ..., w^{T-1}\n",
        "      t_idx = torch.arange(T, device=k.device, dtype=k.dtype)\n",
        "      w_powers = w.unsqueeze(0) ** t_idx.unsqueeze(1)  # (T, D)\n",
        "      w_inv_powers = 1.0 / (w_powers + 1e-10)  # (T, D)\n",
        "\n",
        "      # Rescale to remove exponential decay: x_scaled[t] = w^{-t} * x[t]\n",
        "      ekv_scaled = ekv * w_inv_powers.unsqueeze(0)  # (B, T, D)\n",
        "      ek_scaled = ek * w_inv_powers.unsqueeze(0)  # (B, T, D)\n",
        "\n",
        "      # Cumulative sum in scaled domain\n",
        "      ekv_cumsum = torch.cumsum(ekv_scaled, dim=1)  # (B, T, D)\n",
        "      ek_cumsum = torch.cumsum(ek_scaled, dim=1)  # (B, T, D)\n",
        "\n",
        "      # Shift right to get \"previous\" accumulated state (exclusive prefix sum)\n",
        "      zeros = torch.zeros(B, 1, D, device=k.device, dtype=k.dtype)\n",
        "      ekv_cumsum_prev = torch.cat([zeros, ekv_cumsum[:, :-1, :]], dim=1)\n",
        "      ek_cumsum_prev = torch.cat([zeros, ek_cumsum[:, :-1, :]], dim=1)\n",
        "\n",
        "      # Rescale back by w^{t-1} (shifted powers)\n",
        "      w_powers_prev = torch.cat([torch.ones(1, D, device=k.device, dtype=k.dtype), w_powers[:-1, :]], dim=0)\n",
        "      a_prev = ekv_cumsum_prev * w_powers_prev.unsqueeze(0)  # (B, T, D)\n",
        "      b_prev = ek_cumsum_prev * w_powers_prev.unsqueeze(0)  # (B, T, D)\n",
        "\n",
        "      # Final WKV: (a_prev + u * ek * v) / (b_prev + u * ek + eps)\n",
        "      numer = a_prev + u * ekv\n",
        "      denom = b_prev + u * ek + 1e-8\n",
        "      wkv = numer / denom\n",
        "\n",
        "      return wkv\n",
        "\n",
        "\n",
        "class Student(nn.Module):\n",
        "  def __init__(self, config):\n",
        "      super().__init__()\n",
        "      self.config = config\n",
        "\n",
        "      self.transformer = nn.ModuleDict(dict(\n",
        "          wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "          wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "          drop = nn.Dropout(config.dropout),\n",
        "          h = nn.ModuleList([RWKVBlock(config, layer_id=i) for i in range(config.n_layer)]),\n",
        "          ln_f = nn.LayerNorm(config.n_embd),\n",
        "      ))\n",
        "      self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "      self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "      self.apply(self._init_weights)\n",
        "      print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "  def get_num_params(self, non_embedding=True):\n",
        "      n_params = sum(p.numel() for p in self.parameters())\n",
        "      if non_embedding:\n",
        "          n_params -= self.transformer.wpe.weight.numel()\n",
        "      return n_params\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "      if isinstance(module, nn.Linear):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "          if module.bias is not None:\n",
        "              torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "      device = idx.device\n",
        "      b, t = idx.size()\n",
        "      assert t <= self.config.block_size\n",
        "      pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "\n",
        "      tok_emb = self.transformer.wte(idx)\n",
        "      pos_emb = self.transformer.wpe(pos)\n",
        "      x = self.transformer.drop(tok_emb + pos_emb)\n",
        "\n",
        "      for block in self.transformer.h:\n",
        "          x = block(x)\n",
        "\n",
        "      x = self.transformer.ln_f(x)\n",
        "\n",
        "      if targets is not None:\n",
        "          logits = self.lm_head(x)\n",
        "          loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "      else:\n",
        "          logits = self.lm_head(x[:, [-1], :])\n",
        "          loss = None\n",
        "\n",
        "      return logits, loss\n",
        "\n",
        "  def crop_block_size(self, block_size):\n",
        "      assert block_size <= self.config.block_size\n",
        "      self.config.block_size = block_size\n",
        "      self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
        "\n",
        "  def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "      param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
        "      decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "      nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "\n",
        "      optim_groups = [\n",
        "          {'params': decay_params, 'weight_decay': weight_decay},\n",
        "          {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "      ]\n",
        "\n",
        "      num_decay_params = sum(p.numel() for p in decay_params)\n",
        "      num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "      print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "      print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "\n",
        "      fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "      use_fused = fused_available and device_type == 'cuda'\n",
        "      extra_args = dict(fused=True) if use_fused else dict()\n",
        "      optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "      print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "      return optimizer\n",
        "\n",
        "  def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "      N = self.get_num_params()\n",
        "      cfg = self.config\n",
        "      flops_per_token = 6 * N\n",
        "      flops_per_fwdbwd = flops_per_token * cfg.block_size\n",
        "      flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "      flops_achieved = flops_per_iter * (1.0/dt)\n",
        "      flops_promised = 312e12\n",
        "      mfu = flops_achieved / flops_promised\n",
        "      return mfu\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "      for _ in range(max_new_tokens):\n",
        "          idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "          logits, _ = self(idx_cond)\n",
        "          logits = logits[:, -1, :] / temperature\n",
        "          if top_k is not None:\n",
        "              v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "              logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          idx_next = torch.multinomial(probs, num_samples=1)\n",
        "          idx = torch.cat((idx, idx_next), dim=1)\n",
        "      return idx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1BMoRt83zKL",
        "outputId": "b77bf9e8-a550-490d-ae84-2cec6d4bf877"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing student_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config/train_shakespeare_student.py\n",
        "out_dir = 'out-shakespeare-student'\n",
        "eval_interval = 250\n",
        "eval_iters = 200\n",
        "log_interval = 10\n",
        "\n",
        "always_save_checkpoint = False\n",
        "wandb_log = False\n",
        "\n",
        "dataset = 'shakespeare_char'\n",
        "gradient_accumulation_steps = 1\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "\n",
        "n_layer = 5\n",
        "n_embd = 128\n",
        "dropout = 0.1\n",
        "\n",
        "model_type = 'student'\n",
        "\n",
        "learning_rate = 1e-3\n",
        "max_iters = 5000\n",
        "lr_decay_iters = 5000\n",
        "min_lr = 1e-4\n",
        "beta2 = 0.99\n",
        "warmup_iters = 100\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNw50JKY38hm",
        "outputId": "1c5c9609-11ca-43dc-938a-f3244645b3d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config/train_shakespeare_student.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Student model import and support\n",
        "import fileinput\n",
        "import sys\n",
        "\n",
        "# Read train.py\n",
        "with open('train.py', 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Add import\n",
        "content = content.replace(\n",
        "    \"from model import GPTConfig, GPT\",\n",
        "    \"from model import GPTConfig, GPT\\nfrom student_model import StudentConfig, Student\"\n",
        ")\n",
        "\n",
        "# Add model_type default\n",
        "content = content.replace(\n",
        "    \"bias = False # do we use bias inside LayerNorm and Linear layers?\",\n",
        "    \"bias = False # do we use bias inside LayerNorm and Linear layers?\\nmodel_type = 'gpt' # 'gpt' or 'student'\"\n",
        ")\n",
        "\n",
        "# Add Student model init\n",
        "old_init = '''if init_from == 'scratch':\n",
        "    # init a new model from scratch\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    # determine the vocab size we'll use for from-scratch training\n",
        "    if meta_vocab_size is None:\n",
        "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)'''\n",
        "\n",
        "new_init = '''if init_from == 'scratch':\n",
        "    # init a new model from scratch\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    # determine the vocab size we'll use for from-scratch training\n",
        "    if meta_vocab_size is None:\n",
        "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    if model_type == 'student':\n",
        "        student_args = {k: v for k, v in model_args.items() if k in ['n_layer', 'n_embd', 'block_size', 'vocab_size', 'dropout']}\n",
        "        conf = StudentConfig(**student_args)\n",
        "        model = Student(conf)\n",
        "    else:\n",
        "        gptconf = GPTConfig(**model_args)\n",
        "        model = GPT(gptconf)'''\n",
        "\n",
        "content = content.replace(old_init, new_init)\n",
        "\n",
        "content = content.replace(\n",
        "    \"if iter_num % eval_interval == 0 and master_process:\",\n",
        "    \"if iter_num > 0 and iter_num % eval_interval == 0 and master_process:\"\n",
        ")\n",
        "\n",
        "with open('train.py', 'w') as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"train.py patched!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kna41px34A_b",
        "outputId": "aaeaf7e6-2b42-4f25-8559-4c1038f9a7d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.py patched!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sample.py\n",
        "\"\"\"\n",
        "Sample from a trained model\n",
        "\"\"\"\n",
        "import os\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "import torch\n",
        "import tiktoken\n",
        "from model import GPTConfig, GPT\n",
        "from student_model import StudentConfig, Student\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "init_from = 'resume'\n",
        "out_dir = 'out-shakespeare-student'\n",
        "start = \"\\n\"\n",
        "num_samples = 3\n",
        "max_new_tokens = 500\n",
        "temperature = 0.8\n",
        "top_k = 200\n",
        "seed = 1337\n",
        "device = 'cuda'\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "compile = False\n",
        "exec(open('configurator.py').read())\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# model\n",
        "if init_from == 'resume':\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    model_args = checkpoint['model_args']\n",
        "\n",
        "    # Detect model type\n",
        "    model_type = checkpoint.get('model_type', None)\n",
        "    if model_type is None:\n",
        "        state_keys = list(checkpoint['model'].keys())\n",
        "        model_type = 'student' if any('time_decay' in k for k in state_keys) else 'gpt'\n",
        "\n",
        "    if model_type == 'student':\n",
        "        student_args = {k: v for k, v in model_args.items() if k in ['n_layer', 'n_embd', 'block_size', 'vocab_size', 'dropout']}\n",
        "        conf = StudentConfig(**student_args)\n",
        "        model = Student(conf)\n",
        "    else:\n",
        "        gptconf = GPTConfig(**model_args)\n",
        "        model = GPT(gptconf)\n",
        "\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "elif init_from.startswith('gpt2'):\n",
        "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model)\n",
        "\n",
        "# look for the meta pickle in case it is available in the dataset folder\n",
        "load_meta = False\n",
        "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']:\n",
        "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
        "    load_meta = os.path.exists(meta_path)\n",
        "if load_meta:\n",
        "    print(f\"Loading meta from {meta_path}...\")\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    stoi, itos = meta['stoi'], meta['itos']\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "    decode = lambda l: ''.join([itos[i] for i in l])\n",
        "else:\n",
        "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "    decode = lambda l: enc.decode(l)\n",
        "\n",
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZC8vQWfMyYk",
        "outputId": "0cd7514b-5f60-4d9f-f4a0-e9d2ad30a699"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sample.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/shakespeare_char/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAYVtrCx4BCF",
        "outputId": "91695f44-0e21-4e90-bce2-66b3d025f023"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Models"
      ],
      "metadata": {
        "id": "CDyXxpKMNAqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_student.py --device=cuda --compile=False --log_interval=10 --max_iters=1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhfcM1rl4Gts",
        "outputId": "fd14899b-9ac7-4d36-ebfa-578b80e8afa1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_student.py:\n",
            "out_dir = 'out-shakespeare-student'\n",
            "eval_interval = 250\n",
            "eval_iters = 200\n",
            "log_interval = 10\n",
            "\n",
            "always_save_checkpoint = False\n",
            "wandb_log = False\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256\n",
            "\n",
            "n_layer = 5\n",
            "n_embd = 128\n",
            "dropout = 0.1\n",
            "\n",
            "model_type = 'student'\n",
            "\n",
            "learning_rate = 1e-3\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000\n",
            "min_lr = 1e-4\n",
            "beta2 = 0.99\n",
            "warmup_iters = 100\n",
            "\n",
            "Overriding: device = cuda\n",
            "Overriding: compile = False\n",
            "Overriding: log_interval = 10\n",
            "Overriding: max_iters = 1000\n",
            "tokens per iteration will be: 16,384\n",
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 0.83M\n",
            "/content/nanoGPT/train.py:203: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 62, with 863,488 parameters\n",
            "num non-decayed parameter tensors: 32, with 4,096 parameters\n",
            "using fused AdamW: True\n",
            "iter 0: loss 4.2015, time 1268.72ms, mfu -100.00%\n",
            "iter 10: loss 3.8352, time 151.44ms, mfu 0.17%\n",
            "iter 20: loss 3.6142, time 151.49ms, mfu 0.17%\n",
            "iter 30: loss 3.4725, time 150.66ms, mfu 0.17%\n",
            "iter 40: loss 3.3713, time 150.82ms, mfu 0.17%\n",
            "iter 50: loss 3.2361, time 151.49ms, mfu 0.17%\n",
            "iter 60: loss 3.0584, time 151.09ms, mfu 0.17%\n",
            "iter 70: loss 2.8731, time 151.71ms, mfu 0.17%\n",
            "iter 80: loss 2.6661, time 151.94ms, mfu 0.17%\n",
            "iter 90: loss 2.5166, time 152.84ms, mfu 0.17%\n",
            "iter 100: loss 2.3916, time 152.17ms, mfu 0.17%\n",
            "iter 110: loss 2.2990, time 150.57ms, mfu 0.17%\n",
            "iter 120: loss 2.2231, time 151.35ms, mfu 0.17%\n",
            "iter 130: loss 2.1524, time 151.99ms, mfu 0.17%\n",
            "iter 140: loss 2.1178, time 152.78ms, mfu 0.17%\n",
            "iter 150: loss 2.0399, time 152.60ms, mfu 0.17%\n",
            "iter 160: loss 2.0111, time 152.61ms, mfu 0.17%\n",
            "iter 170: loss 1.9790, time 160.54ms, mfu 0.17%\n",
            "iter 180: loss 1.9452, time 180.15ms, mfu 0.17%\n",
            "iter 190: loss 1.9120, time 148.17ms, mfu 0.17%\n",
            "iter 200: loss 1.8970, time 149.29ms, mfu 0.17%\n",
            "iter 210: loss 1.8574, time 151.52ms, mfu 0.17%\n",
            "iter 220: loss 1.8703, time 153.10ms, mfu 0.17%\n",
            "iter 230: loss 1.8148, time 155.07ms, mfu 0.17%\n",
            "iter 240: loss 1.7930, time 154.80ms, mfu 0.17%\n",
            "step 250: train loss 1.7152, val loss 1.8520\n",
            "saving checkpoint to out-shakespeare-student\n",
            "iter 250: loss 1.7778, time 14825.46ms, mfu 0.15%\n",
            "iter 260: loss 1.7803, time 153.86ms, mfu 0.16%\n",
            "iter 270: loss 1.7239, time 154.54ms, mfu 0.16%\n",
            "iter 280: loss 1.7213, time 154.50ms, mfu 0.16%\n",
            "iter 290: loss 1.6590, time 154.15ms, mfu 0.16%\n",
            "iter 300: loss 1.7100, time 154.26ms, mfu 0.16%\n",
            "iter 310: loss 1.6387, time 154.66ms, mfu 0.16%\n",
            "iter 320: loss 1.6693, time 153.59ms, mfu 0.16%\n",
            "iter 330: loss 1.6454, time 155.05ms, mfu 0.16%\n",
            "iter 340: loss 1.6263, time 152.14ms, mfu 0.16%\n",
            "iter 350: loss 1.6321, time 155.23ms, mfu 0.16%\n",
            "iter 360: loss 1.6305, time 153.70ms, mfu 0.17%\n",
            "iter 370: loss 1.6299, time 155.72ms, mfu 0.17%\n",
            "iter 380: loss 1.6063, time 154.26ms, mfu 0.17%\n",
            "iter 390: loss 1.6171, time 154.81ms, mfu 0.17%\n",
            "iter 400: loss 1.6120, time 154.54ms, mfu 0.17%\n",
            "iter 410: loss 1.5704, time 155.76ms, mfu 0.17%\n",
            "iter 420: loss 1.5859, time 169.18ms, mfu 0.17%\n",
            "iter 430: loss 1.5782, time 180.33ms, mfu 0.16%\n",
            "iter 440: loss 1.5874, time 155.12ms, mfu 0.16%\n",
            "iter 450: loss 1.5620, time 156.51ms, mfu 0.16%\n",
            "iter 460: loss 1.5375, time 154.61ms, mfu 0.17%\n",
            "iter 470: loss 1.5595, time 155.69ms, mfu 0.17%\n",
            "iter 480: loss 1.5634, time 155.87ms, mfu 0.17%\n",
            "iter 490: loss 1.5525, time 155.96ms, mfu 0.17%\n",
            "step 500: train loss 1.4753, val loss 1.6699\n",
            "saving checkpoint to out-shakespeare-student\n",
            "iter 500: loss 1.5190, time 14814.28ms, mfu 0.15%\n",
            "iter 510: loss 1.5451, time 157.88ms, mfu 0.15%\n",
            "iter 520: loss 1.5091, time 156.46ms, mfu 0.15%\n",
            "iter 530: loss 1.5313, time 158.03ms, mfu 0.15%\n",
            "iter 540: loss 1.4978, time 157.14ms, mfu 0.16%\n",
            "iter 550: loss 1.5029, time 156.21ms, mfu 0.16%\n",
            "iter 560: loss 1.5144, time 157.77ms, mfu 0.16%\n",
            "iter 570: loss 1.4755, time 159.04ms, mfu 0.16%\n",
            "iter 580: loss 1.5190, time 157.54ms, mfu 0.16%\n",
            "iter 590: loss 1.5162, time 160.03ms, mfu 0.16%\n",
            "iter 600: loss 1.4535, time 159.88ms, mfu 0.16%\n",
            "iter 610: loss 1.4879, time 158.10ms, mfu 0.16%\n",
            "iter 620: loss 1.4807, time 157.59ms, mfu 0.16%\n",
            "iter 630: loss 1.4775, time 160.65ms, mfu 0.16%\n",
            "iter 640: loss 1.4848, time 158.46ms, mfu 0.16%\n",
            "iter 650: loss 1.4333, time 155.90ms, mfu 0.16%\n",
            "iter 660: loss 1.4550, time 158.78ms, mfu 0.16%\n",
            "iter 670: loss 1.4678, time 159.51ms, mfu 0.16%\n",
            "iter 680: loss 1.4650, time 161.29ms, mfu 0.16%\n",
            "iter 690: loss 1.4413, time 158.46ms, mfu 0.16%\n",
            "iter 700: loss 1.4579, time 162.00ms, mfu 0.16%\n",
            "iter 710: loss 1.4763, time 157.76ms, mfu 0.16%\n",
            "iter 720: loss 1.4740, time 163.10ms, mfu 0.16%\n",
            "iter 730: loss 1.4415, time 158.32ms, mfu 0.16%\n",
            "iter 740: loss 1.4706, time 143.73ms, mfu 0.17%\n",
            "step 750: train loss 1.3842, val loss 1.5977\n",
            "saving checkpoint to out-shakespeare-student\n",
            "iter 750: loss 1.4203, time 14844.90ms, mfu 0.15%\n",
            "iter 760: loss 1.4392, time 157.54ms, mfu 0.15%\n",
            "iter 770: loss 1.4106, time 156.36ms, mfu 0.15%\n",
            "iter 780: loss 1.4078, time 158.01ms, mfu 0.15%\n",
            "iter 790: loss 1.4108, time 158.67ms, mfu 0.16%\n",
            "iter 800: loss 1.4091, time 156.68ms, mfu 0.16%\n",
            "iter 810: loss 1.4258, time 159.83ms, mfu 0.16%\n",
            "iter 820: loss 1.4418, time 155.64ms, mfu 0.16%\n",
            "iter 830: loss 1.4029, time 157.87ms, mfu 0.16%\n",
            "iter 840: loss 1.4005, time 156.91ms, mfu 0.16%\n",
            "iter 850: loss 1.4020, time 156.45ms, mfu 0.16%\n",
            "iter 860: loss 1.4057, time 158.22ms, mfu 0.16%\n",
            "iter 870: loss 1.4049, time 156.09ms, mfu 0.16%\n",
            "iter 880: loss 1.3979, time 158.23ms, mfu 0.16%\n",
            "iter 890: loss 1.4358, time 155.39ms, mfu 0.16%\n",
            "iter 900: loss 1.3909, time 157.72ms, mfu 0.16%\n",
            "iter 910: loss 1.4040, time 157.55ms, mfu 0.16%\n",
            "iter 920: loss 1.3682, time 156.83ms, mfu 0.16%\n",
            "iter 930: loss 1.3948, time 157.54ms, mfu 0.16%\n",
            "iter 940: loss 1.3952, time 158.30ms, mfu 0.16%\n",
            "iter 950: loss 1.4134, time 157.62ms, mfu 0.16%\n",
            "iter 960: loss 1.3807, time 157.54ms, mfu 0.17%\n",
            "iter 970: loss 1.4150, time 157.05ms, mfu 0.17%\n",
            "iter 980: loss 1.3850, time 159.45ms, mfu 0.17%\n",
            "iter 990: loss 1.3932, time 158.98ms, mfu 0.17%\n",
            "step 1000: train loss 1.3292, val loss 1.5617\n",
            "saving checkpoint to out-shakespeare-student\n",
            "iter 1000: loss 1.3847, time 14766.06ms, mfu 0.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZYECazi95Qv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py --device=cuda --compile=False --log_interval=10 --max_iters=1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Unx1ZCUz4BE8",
        "outputId": "db7b13a6-e25e-4534-8148-ae45c9bddc6c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: device = cuda\n",
            "Overriding: compile = False\n",
            "Overriding: log_interval = 10\n",
            "Overriding: max_iters = 1000\n",
            "tokens per iteration will be: 16,384\n",
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "/content/nanoGPT/train.py:203: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "iter 0: loss 4.2653, time 812.59ms, mfu -100.00%\n",
            "iter 10: loss 3.1365, time 512.76ms, mfu 0.73%\n",
            "iter 20: loss 2.7671, time 514.14ms, mfu 0.73%\n",
            "iter 30: loss 2.6446, time 517.11ms, mfu 0.73%\n",
            "iter 40: loss 2.5474, time 520.73ms, mfu 0.72%\n",
            "iter 50: loss 2.5272, time 526.01ms, mfu 0.72%\n",
            "iter 60: loss 2.4866, time 530.91ms, mfu 0.72%\n",
            "iter 70: loss 2.4784, time 533.66ms, mfu 0.72%\n",
            "iter 80: loss 2.4942, time 535.67ms, mfu 0.72%\n",
            "iter 90: loss 2.4865, time 536.81ms, mfu 0.71%\n",
            "iter 100: loss 2.4596, time 533.16ms, mfu 0.71%\n",
            "iter 110: loss 2.4331, time 529.07ms, mfu 0.71%\n",
            "iter 120: loss 2.4457, time 525.46ms, mfu 0.71%\n",
            "iter 130: loss 2.4259, time 522.84ms, mfu 0.71%\n",
            "iter 140: loss 2.4036, time 521.16ms, mfu 0.71%\n",
            "iter 150: loss 2.3863, time 518.05ms, mfu 0.71%\n",
            "iter 160: loss 2.3666, time 518.65ms, mfu 0.71%\n",
            "iter 170: loss 2.3555, time 522.52ms, mfu 0.71%\n",
            "iter 180: loss 2.3368, time 522.32ms, mfu 0.71%\n",
            "iter 190: loss 2.2711, time 523.39ms, mfu 0.71%\n",
            "iter 200: loss 2.2621, time 523.44ms, mfu 0.71%\n",
            "iter 210: loss 2.1901, time 525.26ms, mfu 0.71%\n",
            "iter 220: loss 2.1507, time 528.94ms, mfu 0.71%\n",
            "iter 230: loss 2.0956, time 525.97ms, mfu 0.71%\n",
            "iter 240: loss 2.0920, time 527.41ms, mfu 0.71%\n",
            "step 250: train loss 1.9666, val loss 2.0838\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.0272, time 74284.78ms, mfu 0.64%\n",
            "iter 260: loss 2.0133, time 522.37ms, mfu 0.65%\n",
            "iter 270: loss 1.9967, time 523.33ms, mfu 0.65%\n",
            "iter 280: loss 1.9802, time 525.37ms, mfu 0.66%\n",
            "iter 290: loss 1.9543, time 523.26ms, mfu 0.66%\n",
            "iter 300: loss 1.8903, time 523.36ms, mfu 0.67%\n",
            "iter 310: loss 1.8899, time 524.01ms, mfu 0.67%\n",
            "iter 320: loss 1.8489, time 525.31ms, mfu 0.68%\n",
            "iter 330: loss 1.8401, time 525.40ms, mfu 0.68%\n",
            "iter 340: loss 1.8281, time 524.88ms, mfu 0.68%\n",
            "iter 350: loss 1.8040, time 524.91ms, mfu 0.69%\n",
            "iter 360: loss 1.7918, time 524.90ms, mfu 0.69%\n",
            "iter 370: loss 1.7703, time 525.62ms, mfu 0.69%\n",
            "iter 380: loss 1.7454, time 525.81ms, mfu 0.69%\n",
            "iter 390: loss 1.7152, time 525.46ms, mfu 0.69%\n",
            "iter 400: loss 1.7300, time 523.43ms, mfu 0.70%\n",
            "iter 410: loss 1.7066, time 526.29ms, mfu 0.70%\n",
            "iter 420: loss 1.6936, time 524.57ms, mfu 0.70%\n",
            "iter 430: loss 1.6584, time 525.67ms, mfu 0.70%\n",
            "iter 440: loss 1.6907, time 526.68ms, mfu 0.70%\n",
            "iter 450: loss 1.6631, time 524.82ms, mfu 0.70%\n",
            "iter 460: loss 1.6784, time 525.77ms, mfu 0.70%\n",
            "iter 470: loss 1.6631, time 525.72ms, mfu 0.70%\n",
            "iter 480: loss 1.6336, time 527.10ms, mfu 0.70%\n",
            "iter 490: loss 1.6288, time 524.33ms, mfu 0.70%\n",
            "step 500: train loss 1.5331, val loss 1.7263\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.6185, time 74503.25ms, mfu 0.63%\n",
            "iter 510: loss 1.6161, time 522.62ms, mfu 0.64%\n",
            "iter 520: loss 1.5707, time 523.82ms, mfu 0.65%\n",
            "iter 530: loss 1.5927, time 524.99ms, mfu 0.65%\n",
            "iter 540: loss 1.5554, time 524.73ms, mfu 0.66%\n",
            "iter 550: loss 1.5856, time 525.75ms, mfu 0.67%\n",
            "iter 560: loss 1.5529, time 525.58ms, mfu 0.67%\n",
            "iter 570: loss 1.5772, time 526.38ms, mfu 0.67%\n",
            "iter 580: loss 1.5655, time 525.93ms, mfu 0.68%\n",
            "iter 590: loss 1.5021, time 525.69ms, mfu 0.68%\n",
            "iter 600: loss 1.5378, time 526.08ms, mfu 0.68%\n",
            "iter 610: loss 1.4808, time 526.15ms, mfu 0.69%\n",
            "iter 620: loss 1.5200, time 526.55ms, mfu 0.69%\n",
            "iter 630: loss 1.4943, time 524.81ms, mfu 0.69%\n",
            "iter 640: loss 1.4811, time 524.55ms, mfu 0.69%\n",
            "iter 650: loss 1.5222, time 525.73ms, mfu 0.69%\n",
            "iter 660: loss 1.5190, time 525.40ms, mfu 0.70%\n",
            "iter 670: loss 1.4245, time 525.55ms, mfu 0.70%\n",
            "iter 680: loss 1.5004, time 525.44ms, mfu 0.70%\n",
            "iter 690: loss 1.4754, time 526.23ms, mfu 0.70%\n",
            "iter 700: loss 1.4845, time 526.89ms, mfu 0.70%\n",
            "iter 710: loss 1.4528, time 522.54ms, mfu 0.70%\n",
            "iter 720: loss 1.4302, time 525.83ms, mfu 0.70%\n",
            "iter 730: loss 1.4263, time 527.26ms, mfu 0.70%\n",
            "iter 740: loss 1.4530, time 524.89ms, mfu 0.70%\n",
            "step 750: train loss 1.3578, val loss 1.5770\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.4380, time 74627.82ms, mfu 0.63%\n",
            "iter 760: loss 1.4202, time 522.87ms, mfu 0.64%\n",
            "iter 770: loss 1.4313, time 524.03ms, mfu 0.65%\n",
            "iter 780: loss 1.4289, time 525.25ms, mfu 0.65%\n",
            "iter 790: loss 1.4385, time 523.42ms, mfu 0.66%\n",
            "iter 800: loss 1.4004, time 523.88ms, mfu 0.67%\n",
            "iter 810: loss 1.4325, time 525.02ms, mfu 0.67%\n",
            "iter 820: loss 1.4167, time 523.37ms, mfu 0.67%\n",
            "iter 830: loss 1.3744, time 523.86ms, mfu 0.68%\n",
            "iter 840: loss 1.4140, time 524.32ms, mfu 0.68%\n",
            "iter 850: loss 1.4068, time 524.33ms, mfu 0.68%\n",
            "iter 860: loss 1.3857, time 524.66ms, mfu 0.69%\n",
            "iter 870: loss 1.3572, time 524.56ms, mfu 0.69%\n",
            "iter 880: loss 1.3740, time 525.99ms, mfu 0.69%\n",
            "iter 890: loss 1.3313, time 527.34ms, mfu 0.69%\n",
            "iter 900: loss 1.3854, time 525.93ms, mfu 0.69%\n",
            "iter 910: loss 1.3971, time 525.40ms, mfu 0.70%\n",
            "iter 920: loss 1.3728, time 523.28ms, mfu 0.70%\n",
            "iter 930: loss 1.3606, time 523.85ms, mfu 0.70%\n",
            "iter 940: loss 1.3215, time 526.84ms, mfu 0.70%\n",
            "iter 950: loss 1.3508, time 525.41ms, mfu 0.70%\n",
            "iter 960: loss 1.3436, time 526.79ms, mfu 0.70%\n",
            "iter 970: loss 1.3263, time 525.20ms, mfu 0.70%\n",
            "iter 980: loss 1.3507, time 524.73ms, mfu 0.70%\n",
            "iter 990: loss 1.3370, time 525.82ms, mfu 0.70%\n",
            "step 1000: train loss 1.2712, val loss 1.5193\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3153, time 74444.93ms, mfu 0.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample the Models"
      ],
      "metadata": {
        "id": "hhzXV2PNM5F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-shakespeare-student --start=\"ROMEO:\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1i4UQDk-aOw",
        "outputId": "3d7c413b-f10d-44b0-d099-a6565c7a2cc2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-student\n",
            "Overriding: start = ROMEO:\n",
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n",
            "number of parameters: 0.83M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "ROMEO:\n",
            "You shall you will be your some is to Barnar:\n",
            "What then? what's he would yet will have?\n",
            "\n",
            "MERCUTIO:\n",
            "Why crown?\n",
            "\n",
            "PRINCE EDWARD:\n",
            "You shall be a come mile will, and it end.\n",
            "\n",
            "LUCIO:\n",
            "Your drop the countreater of you so. Ah, no mean.\n",
            "\n",
            "KING RICHARD III:\n",
            "Marry, let me love.\n",
            "\n",
            "RICHARD:\n",
            "Ay, sir, thy world that more not what evil sound,\n",
            "For this cheek me strong of his guilt and thrust,\n",
            "Is no worse my mother all of which Prince,\n",
            "So murder some still is a hope of the hearth,\n",
            "Which as I tear the such not carel\n",
            "---------------\n",
            "ROMEO:\n",
            "Mine you shall my soul that you so you myself and\n",
            "The world madam stand against the world.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Why, castle. And you not peace thou ever\n",
            "About over the pass'd of this moved by mind the heart;\n",
            "Which of you to prove his heart, if I content me\n",
            "As wantone her against the way soul.\n",
            "\n",
            "Clown:\n",
            "For these lady, I will with a country's mine.\n",
            "\n",
            "PAULINA:\n",
            "And Catter, so I soon, if out our gentleman.\n",
            "\n",
            "CAMILLO:\n",
            "Sister thank the silent well be a punish and refused\n",
            "By thine.\n",
            "\n",
            "ESCALUS:\n",
            "O must one which \n",
            "---------------\n",
            "ROMEO:\n",
            "Then, my lord,\n",
            "That, I have no more her men in the princess;\n",
            "Than you will shall professess thinks and ends\n",
            "Till your son his mine and which his body.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Why, that I see he and inkeen doth his current,\n",
            "Reing breath is the weep with mine of your life,\n",
            "His heaven likely of grace of the others\n",
            "Hath something the father, makes your shall die.\n",
            "\n",
            "Provoded:\n",
            "So the vain himself:\n",
            "He was not but no more, my vice else of me.\n",
            "Whither worthmen's friends and light is our queen,\n",
            "Revengel have \n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-shakespeare-char --start=\"ROMEO:\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0FII5w6-c12",
        "outputId": "2567ab0d-1644-46a2-a141-17150fd215e0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-char\n",
            "Overriding: start = ROMEO:\n",
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "ROMEO:\n",
            "All the bride will the comple\n",
            "And the way down will desire to a\n",
            "heaven thee us are to be determitted with my father,\n",
            "And I must be proof in heart my heart.\n",
            "\n",
            "CLIFFORD:\n",
            "In ministress in the son, and let him be joy\n",
            "Well not peace thee won him from than speak you:\n",
            "That I may perform to be the that that more not when evil\n",
            "To the trouble hand him from of my heart\n",
            "To known breathe son; if he must not hale got forget.\n",
            "\n",
            "CAMILLO:\n",
            "Why, so hard you saw his brother'd state: but knew and\n",
            "His hand, and for hi\n",
            "---------------\n",
            "ROMEO:\n",
            "One to the marriage of the measure,\n",
            "Before the noble two that take the crown\n",
            "But not us but the much friends.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "And let me stronged the country's death?\n",
            "\n",
            "LUCIO:\n",
            "For over some so under in meetimage and come,\n",
            "Against his wife his view, or I change beauty against thee;\n",
            "And she they long me to commend me in her speak.\n",
            "\n",
            "PARIS:\n",
            "I know thee were do thou may to do it is determited.\n",
            "\n",
            "GLOUCESTER:\n",
            "And the bent of the Curiest Blunt, marry, thy gracious crown,\n",
            "And thou steel for Gloucester's \n",
            "---------------\n",
            "ROMEO:\n",
            "The can be well, as I can she is heard, the cares of me\n",
            "flatter, when we have against thee: and the king worse\n",
            "find the friends Warwick had come to the\n",
            "counsel and him to their care ages.\n",
            "\n",
            "LADY CAPULET:\n",
            "I mean they say think by men's mistresses of grief,\n",
            "That they will be that Turk brook'd the old constanted defend,\n",
            "Scrifits of this and be love in still belowing on the duke:\n",
            "I have resorved to prone, the town, mind that she been\n",
            "So in the pronounce of enemy.\n",
            "\n",
            "GLOUCESTER:\n",
            "Meantime are will have \n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IiPg5bkoQfO1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}